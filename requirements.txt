Here's the updated README with a comprehensive overview of the BLIP3-o Enhanced Patch-Level DiT implementation:

```markdown
# BLIP3-o Enhanced Patch-Level DiT: Image-to-Text Translation via Flow Matching

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TD
    A[Input Images] --> B[EVA-CLIP Encoder]
    A --> C[CLIP ViT Encoder]
    
    B --> D["EVA Features<br>(B, 257, 4096)"]
    C --> E["CLIP Features<br>(B, 257, 1024)"]
    
    D --> F["Cross-Attention<br>Conditioning"]
    E --> G["Flow Matching Target"]
    
    H["Noise<br>(B, 257, 1024)"] --> I["Linear Interpolation<br>xâ‚œ = (1-Î±)xâ‚€ + Î±xâ‚"]
    G --> I
    
    I --> J["BLIP3-o DiT Model<br>12 Layers, 768 Hidden"]
    F --> J
    
    J --> K["Velocity Prediction<br>(B, 257, 1024)"]
    
    K --> L["Flow Matching Loss<br>MSE(v_pred, v_target)"]
    G --> L
    
    style J fill:#e1f5fe
    style L fill:#ffebee
    style F fill:#f3e5f5
```

### Core Components:
1. **EVA-CLIP Encoder**: Extracts 4096-dimensional features (257 tokens per image)
2. **CLIP ViT Encoder**: Extracts 1024-dimensional patch embeddings (257 tokens)
3. **Linear Interpolation**: Creates noisy inputs for flow matching
4. **DiT Model**: 12-layer transformer with:
   - Rotary position embeddings
   - Cross-attention conditioning
   - Gradient checkpointing support
5. **Flow Matching Loss**: Pure velocity prediction (BLIP3-o paper aligned)

## ğŸ“¥ Embedding Extraction

### Extract CLS+Patch Embeddings (257 tokens):
```bash
python src/modules/extract_embeddings_g.py \
    --output_dir "./embeddings/cls_patch_257" \
    --training_mode "cls_patch" \
    --batch_size 32
```

### Extract Patch-Only Embeddings (256 tokens):
```bash
python src/modules/extract_embeddings_g.py \
    --output_dir "./embeddings/patch_only_256" \
    --training_mode "patch_only" \
    --batch_size 64
```

## ğŸš€ Training Commands

### CLS+Patch Training (257 tokens):
```bash
python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "./embeddings/cls_patch_257" \
    --output_dir "./checkpoints/cls_patch" \
    --training_mode "cls_patch" \
    --num_epochs 10 \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 2 \
    --enable_detailed_eval
```

### Patch-Only Training (256 tokens):
```bash
python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "./embeddings/patch_only_256" \
    --output_dir "./checkpoints/patch_only" \
    --training_mode "patch_only" \
    --num_epochs 10 \
    --batch_size 16 \
    --learning_rate 2e-4 \
    --gradient_accumulation_steps 4 \
    --enable_same_data_eval
```

### Overfitting Test (Single Shard):
```bash
python train_blip3o_enhanced.py \
    --max_training_shards 1 \
    --overfitting_test \
    --same_data_eval_frequency 50
```

## ğŸ“Š Evaluation & Analysis

```bash
python eval_blip3o_patch_similarity.py \
    --model_path "./checkpoints/cls_patch" \
    --embeddings_dir "./embeddings/cls_patch_257" \
    --output_dir "./results" \
    --num_samples 5000 \
    --save_plots \
    --compute_recall
```

## âš™ï¸ Model Configuration

### BLIP3oDiTConfig Parameters:
```python
config = {
    "hidden_size": 768,           # Model dimension
    "num_hidden_layers": 12,      # Transformer layers
    "num_attention_heads": 12,    # Attention heads
    "eva_embedding_size": 4096,   # EVA feature dimension
    "clip_embedding_size": 1024,  # CLIP feature dimension
    "num_tokens": 257,            # 256 or 257 tokens
    "prediction_type": "velocity" # Flow matching target
}
```

### Flow Matching Loss:
```python
loss_fn = BLIP3oFlowMatchingLoss(
    sigma_min=1e-4,               # Minimum noise level
    sigma_max=1.0,                # Maximum noise level
    normalize_targets=True,        # Normalize CLIP targets
    prediction_type="velocity"     # Velocity prediction
)
```

## ğŸ“ Project Structure
```
blip3o-enhanced/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ blip3o_patch_dit.py           # DiT model implementation
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ blip3o_flow_matching_loss.py  # Pure flow matching loss
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ blip3o_dataset.py             # Token-mode aware dataloader
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â””â”€â”€ blip3o_flexible_trainer.py    # Enhanced training loop
â”‚   â””â”€â”€ extraction/
â”‚       â””â”€â”€ extract_embeddings_g.py       # Embedding extraction
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_blip3o_enhanced.py          # Main training script
â”‚   â””â”€â”€ eval_patch_similarity.py          # Evaluation script
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ blip3o_config.py                  # Model configurations
â””â”€â”€ README.md
```

## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Work
- [BLIP3-o Paper](https://arxiv.org/abs/your-paper-id)
- [EVA-CLIP](https://github.com/baaivision/EVA)
- [Flow Matching](https://arxiv.org/abs/2210.02747)
```

