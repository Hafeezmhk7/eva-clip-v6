# EVA-CLIP to CLIP Flow Matching for 3D Vision

A research project implementing flow matching to bridge EVA-CLIP and CLIP L-14 representation spaces for downstream 3D model applications.

## ğŸ¯ Project Overview

This project develops a trainable pipeline that maps EVA-CLIP image representations to CLIP ViT-L/14 feature space using flow matching with Lumina-Next. The goal is to leverage EVA-CLIP's superior visual encoding while maintaining compatibility with existing 3D models built on CLIP L-14 features.

### Architecture

```
Image â†’ EVA-CLIP L-14 â†’ [Cross-Attention] â†’ Lumina-Next DiT â†’ CLIP ViT-L/14 Features
                                â†‘
                              Noise â†’ Flow Matching
```

## ğŸ—ï¸ Technical Approach

- **Source Encoder**: EVA-CLIP L-14 for robust visual feature extraction
- **Target Space**: CLIP ViT-L/14 (768-dim) for 3D model compatibility  
- **Flow Matching**: Lumina-Next DiT architecture with cross-attention conditioning
- **Training**: Flow matching loss between predicted and ground truth CLIP features

## ğŸ“‚ Project Structure

```
eva-clip-flow-matching/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                 # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ models/               # Model implementations
â”‚   â”œâ”€â”€ training/             # Training pipeline
â”‚   â””â”€â”€ evaluation/           # Metrics and evaluation
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ scripts/                  # Utility scripts
â”œâ”€â”€ notebooks/                # Exploration and analysis
â””â”€â”€ cache/                    # Cached features (not in git)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- Access to Snellius computing cluster (for dataset storage)
- CUDA-capable GPU (recommended)

### Installation

```bash
git clone https://github.com/your-org/eva-clip-flow-matching.git
cd eva-clip-flow-matching
pip install -r requirements.txt
```

### Dataset Setup

We use the BLIP3o-pretrain-short-caption dataset. For initial development, download one shard:

```bash
# On Snellius
cd /path/to/your/data
python scripts/download_data.py --shard 0 --num_samples 1000
```

### Feature Caching

Pre-compute and cache EVA-CLIP and CLIP features for faster training:

```bash
python scripts/cache_features.py --dataset_path /path/to/data --shard 0
```

### Training

```bash
python src/training/train.py --config config/model_config.yaml
```

## ğŸ“Š Current Status

- âœ… **Phase 1**: Data pipeline and feature extraction setup
- ğŸ”„ **Phase 2**: Lumina-Next implementation (in progress)
- â³ **Phase 3**: Training pipeline and evaluation
- â³ **Phase 4**: Integration with 3D model pipeline

## ğŸ”§ Development Notes

### Model Specifications

- **EVA-CLIP L-14**: 768-dimensional image features
- **CLIP ViT-L/14**: 768-dimensional target features  
- **Lumina-Next**: DiT-based flow matching model
- **Flow Matching**: Continuous normalizing flows for representation mapping

### Key Design Decisions

1. **Feature Caching**: Pre-compute embeddings to accelerate training iterations
2. **Single Shard Training**: Start with 1K samples for rapid prototyping
3. **Cross-Attention**: Use EVA features to condition the flow matching process
4. **Modular Architecture**: Separate data, models, and training for easy experimentation

## ğŸ“– References

- [EVA: Exploring the Limits of Masked Visual Representation Learning at Scale](https://arxiv.org/abs/2211.07636)
- [Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT](https://arxiv.org/abs/2406.18583)
- [BLIP-3: Building Large-scale Vision-Language Models](https://arxiv.org/abs/2408.11060)

## ğŸ‘¥ Team

- **Graduate Student**: [Mohammad Hafeez Khan] - Pipeline implementation 


## ğŸ”¬ Research Context

This work is part of a larger effort to build unified 3D models that can leverage multiple vision-language representations. By creating learnable mappings between different encoder spaces, we aim to combine the strengths of various models while maintaining compatibility with existing pipelines.

## ğŸ“ License

[Add your institution's license here]

## ğŸ¤ Contributing

This is an active research project. For questions or collaboration:
- Open an issue for bugs or feature requests
- Contact the team for research collaboration opportunities

---

**Note**: This project is in active development. Documentation and code will be updated frequently as we progress through the implementation phases.
