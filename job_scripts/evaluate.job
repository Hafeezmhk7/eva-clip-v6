#!/bin/bash
#SBATCH --job-name=blip3o_evaluation_fair
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --cpus-per-gpu=18
#SBATCH --time=4:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/evaluation_%j.out
#SBATCH --error=./slurm_out/evaluation_%j.err

echo "ðŸ” BLIP3-o DiT Model Evaluation on MS-COCO - UPDATED WITH FAIR COMPARISON"
echo "=========================================================================="
echo "ðŸŽ¯ IMPROVEMENT: Now uses CLIP's visual projection for fair comparison!"
echo "Both methods compared in CLIP's aligned 768-dimensional embedding space"
echo ""
echo "Tasks:"
echo "  ðŸ“Š Task 1: Alignment Evaluation (Cosine Similarity in CLIP-aligned space)"
echo "  ðŸŽ¯ Task 2: Recall Evaluation (Image-to-Text Retrieval in CLIP-aligned space)"
echo "=========================================================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "=========================================================================="

# =============================================================================
# ENVIRONMENT SETUP (Same as training script)
# =============================================================================

echo ""
echo "ðŸ”§ Environment Setup..."
echo "======================="

# Load modules (same as training)
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Environment check
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'Devices: {torch.cuda.device_count()}')"
echo "========================="

# Check GPU availability
if [ $(python -c "import torch; print(torch.cuda.device_count())") -eq 0 ]; then
    echo "âŒ No GPUs available! Check SLURM allocation."
    exit 1
fi

echo "ðŸŽ® GPU setup:"
echo "   GPUs available: $(python -c "import torch; print(torch.cuda.device_count())")"
echo "   GPU name: $(python -c "import torch; print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')")"

# Set up directories (same as training)
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp for evaluation results
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_eval_${BLIP3O_JOB_ID}"

# Create directories
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,results}

# Redirect model caches to temp (avoid quota issues)
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

# Create cache directories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "âœ… Environment configured"
echo "   Workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"

# =============================================================================
# MODEL AND DATA PATHS
# =============================================================================

echo ""
echo "ðŸ“ Setting up paths..."
echo "======================"

# Model path - UPDATE THIS to your specific model
MODEL_PATH="/scratch-shared/scur2711/blip3o_workspace/checkpoints/blip3o_multi_gpu_fixed_cosine_13170504_20250716_034251"

# COCO dataset path
COCO_ROOT="./data/coco"

# Results directory
RESULTS_DIR="${BLIP3O_JOB_TEMP}/results"
PERSISTENT_RESULTS_DIR="${BLIP3O_WORKSPACE}/evaluation_results/eval_fair_${BLIP3O_JOB_ID}_$(date +%Y%m%d_%H%M%S)"

# Create results directories
mkdir -p "$RESULTS_DIR"
mkdir -p "$PERSISTENT_RESULTS_DIR"

# Verify paths
echo "ðŸ” Verifying paths..."
if [ ! -d "$MODEL_PATH" ]; then
    echo "âŒ Model not found: $MODEL_PATH"
    echo "Available models:"
    ls -la "${BLIP3O_CHECKPOINTS}/" | grep blip3o || echo "No models found"
    exit 1
fi

if [ ! -d "$COCO_ROOT" ]; then
    echo "âŒ COCO dataset not found: $COCO_ROOT"
    echo "Please ensure COCO is downloaded in ./data/coco/"
    exit 1
fi

echo "âœ… Model found: $MODEL_PATH"
echo "âœ… COCO found: $COCO_ROOT"
echo "âœ… Results will be saved to: $PERSISTENT_RESULTS_DIR"

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================

echo ""
echo "âš™ï¸  Evaluation Configuration..."
echo "=============================="

# Evaluation parameters
MAX_SAMPLES=0       # Number of samples to evaluate (set to null for all)
BATCH_SIZE=64           # Batch size for GPU evaluation
DEVICE="cuda"           # Use GPU
VERBOSE_FLAG="--verbose"

# For testing, use smaller values
if [ "${1:-}" = "test" ]; then
    echo "ðŸ§ª TEST MODE: Using small sample size"
    MAX_SAMPLES=20
    BATCH_SIZE=4
fi

echo "ðŸ“Š Evaluation settings:"
echo "   Max samples: $MAX_SAMPLES"
echo "   Batch size: $BATCH_SIZE"
echo "   Device: $DEVICE"
echo "   Model: $(basename $MODEL_PATH)"
echo ""
echo "ðŸŽ¯ Evaluation Improvements:"
echo "   âœ… Uses CLIP's visual projection for fair comparison"
echo "   âœ… Both methods compared in CLIP's aligned 768-dim space"
echo "   âœ… Follows CLIP's standard evaluation methodology"
echo "   âœ… Literature-compliant baseline established"

# =============================================================================
# TASK 1: ALIGNMENT EVALUATION
# =============================================================================

echo ""
echo "ðŸ“Š Starting Task 1: Alignment Evaluation (Fair Comparison)"
echo "=========================================================="
echo "ðŸŽ¯ UPDATED: Now evaluating alignment in CLIP's aligned embedding space"
echo ""
echo "Method comparison:"
echo "  (a) CLIP text + CLIP vision â†’ visual projection â†’ 768-dim aligned space"
echo "  (b) CLIP text + Generated CLIP (EVAâ†’BLIP3-o) â†’ visual projection â†’ 768-dim aligned space"
echo ""
echo "âœ… Fair comparison: Both methods use same embedding space"
echo "âœ… Literature compliant: Follows CLIP's evaluation methodology"

ALIGNMENT_START_TIME=$(date +%s)

python evaluate_alignment.py \
    --blip3o_model_path "$MODEL_PATH" \
    --coco_root "$COCO_ROOT" \
    --max_samples $MAX_SAMPLES \
    --batch_size $BATCH_SIZE \
    --device $DEVICE \
    --results_dir "${RESULTS_DIR}/alignment" \
    --save_detailed \
    $VERBOSE_FLAG

ALIGNMENT_EXIT_CODE=$?
ALIGNMENT_END_TIME=$(date +%s)
ALIGNMENT_DURATION=$((ALIGNMENT_END_TIME - ALIGNMENT_START_TIME))

if [ $ALIGNMENT_EXIT_CODE -eq 0 ]; then
    echo "âœ… Task 1 (Alignment) completed successfully in ${ALIGNMENT_DURATION}s"
    echo "   Results reflect fair comparison in CLIP's aligned space"
else
    echo "âŒ Task 1 (Alignment) failed with exit code: $ALIGNMENT_EXIT_CODE"
fi

# =============================================================================
# TASK 2: RECALL EVALUATION  
# =============================================================================

echo ""
echo "ðŸŽ¯ Starting Task 2: Recall Evaluation (Fair Comparison)"
echo "======================================================="
echo "ðŸŽ¯ UPDATED: Now evaluating retrieval in CLIP's aligned embedding space"
echo ""
echo "Method comparison:"
echo "  (a) Image â†’ CLIP vision â†’ visual projection â†’ retrieval (768-dim aligned)"
echo "  (b) Image â†’ EVA-CLIP â†’ BLIP3-o â†’ visual projection â†’ retrieval (768-dim aligned)"
echo ""
echo "âœ… Fair retrieval: Both image methods compete in same space against text"
echo "âœ… Meaningful results: Differences reflect actual model performance"

RECALL_START_TIME=$(date +%s)

python evaluate_recall.py \
    --blip3o_model_path "$MODEL_PATH" \
    --coco_root "$COCO_ROOT" \
    --max_samples $MAX_SAMPLES \
    --batch_size $BATCH_SIZE \
    --k_values 1 5 10 \
    --device $DEVICE \
    --results_dir "${RESULTS_DIR}/recall" \
    --save_detailed \
    $VERBOSE_FLAG

RECALL_EXIT_CODE=$?
RECALL_END_TIME=$(date +%s)
RECALL_DURATION=$((RECALL_END_TIME - RECALL_START_TIME))

if [ $RECALL_EXIT_CODE -eq 0 ]; then
    echo "âœ… Task 2 (Recall) completed successfully in ${RECALL_DURATION}s"
    echo "   Results reflect fair comparison in CLIP's aligned space"
else
    echo "âŒ Task 2 (Recall) failed with exit code: $RECALL_EXIT_CODE"
fi

# =============================================================================
# RESULTS ARCHIVING
# =============================================================================

echo ""
echo "ðŸ“ Archiving Results..."
echo "======================"

# Copy results to persistent storage
if [ -d "$RESULTS_DIR" ] && [ "$(ls -A "$RESULTS_DIR")" ]; then
    echo "ðŸ“ Copying results to persistent storage..."
    cp -r "$RESULTS_DIR"/* "$PERSISTENT_RESULTS_DIR/"
    
    # Create summary file
    SUMMARY_FILE="$PERSISTENT_RESULTS_DIR/evaluation_summary.txt"
    
    cat > "$SUMMARY_FILE" << EOF
BLIP3-o DiT Evaluation Summary - FAIR COMPARISON VERSION
=========================================================
Job ID: $SLURM_JOB_ID
Date: $(date)
Node: $SLURMD_NODENAME
User: $BLIP3O_USER

ðŸŽ¯ EVALUATION IMPROVEMENTS:
- Uses CLIP's visual projection for fair comparison
- Both methods compared in CLIP's aligned 768-dim space
- Follows CLIP's standard evaluation methodology
- Results reflect actual model performance differences

Model Information:
- Path: $MODEL_PATH
- Model: $(basename $MODEL_PATH)

Evaluation Settings:
- Max samples: $MAX_SAMPLES
- Batch size: $BATCH_SIZE
- Device: $DEVICE
- COCO root: $COCO_ROOT
- Embedding space: CLIP-aligned 768-dimensional
- Uses visual projection: YES

Task Results:
- Task 1 (Alignment): $([ $ALIGNMENT_EXIT_CODE -eq 0 ] && echo "SUCCESS" || echo "FAILED") (${ALIGNMENT_DURATION}s)
- Task 2 (Recall): $([ $RECALL_EXIT_CODE -eq 0 ] && echo "SUCCESS" || echo "FAILED") (${RECALL_DURATION}s)

Results Location: $PERSISTENT_RESULTS_DIR

Files Generated:
$(find "$PERSISTENT_RESULTS_DIR" -type f -name "*.json" | sed 's|^|  - |')
EOF

    echo "âœ… Results saved to: $PERSISTENT_RESULTS_DIR"
    echo "ðŸ“„ Summary saved to: $SUMMARY_FILE"
    
    # Show quick summary
    echo ""
    echo "ðŸ“ˆ Quick Results Preview (Fair Comparison):"
    echo "==========================================="
    
    # Show alignment results if available
    if [ -f "$PERSISTENT_RESULTS_DIR/alignment/alignment_summary.json" ]; then
        echo "ðŸ“Š Alignment Results (768-dim aligned space):"
        python -c "
import json
try:
    with open('$PERSISTENT_RESULTS_DIR/alignment/alignment_summary.json', 'r') as f:
        data = json.load(f)
    print(f'  CLIP Text + CLIP Vision (768-dim):     {data.get(\"clip_text_clip_vision_mean\", 0):.4f}')
    print(f'  CLIP Text + Generated CLIP (768-dim):  {data.get(\"clip_text_generated_mean\", 0):.4f}')
    print(f'  Difference (fair comparison):          {data.get(\"difference_mean\", 0):+.4f}')
    print(f'  Samples evaluated:                     {data.get(\"num_samples\", 0)}')
    print(f'  Embedding space:                       {data.get(\"embedding_space\", \"unknown\")}')
    print(f'  Uses visual projection:                {data.get(\"uses_visual_projection\", False)}')
except Exception as e:
    print(f'  Could not parse alignment results: {e}')
"
    fi
    
    # Show recall results if available
    if [ -f "$PERSISTENT_RESULTS_DIR/recall/recall_summary.json" ]; then
        echo ""
        echo "ðŸŽ¯ Recall Results (768-dim aligned space):"
        python -c "
import json
try:
    with open('$PERSISTENT_RESULTS_DIR/recall/recall_summary.json', 'r') as f:
        data = json.load(f)
    print('  Method (a) - Imageâ†’CLIP Visionâ†’Visual Projâ†’Text:')
    for k in [1, 5, 10]:
        recall = data.get(f'clip_vision_recall@{k}', 0)
        print(f'    Recall@{k:2d}: {recall:.4f} ({recall*100:.2f}%)')
    print('  Method (b) - Imageâ†’EVAâ†’BLIP3oâ†’Visual Projâ†’Text:')
    for k in [1, 5, 10]:
        recall = data.get(f'generated_recall@{k}', 0)
        print(f'    Recall@{k:2d}: {recall:.4f} ({recall*100:.2f}%)')
    print('  Fair comparison improvements:')
    for k in [1, 5, 10]:
        diff = data.get(f'recall@{k}_difference', 0)
        print(f'    Recall@{k:2d}: {diff:+.4f} ({diff*100:+.2f}%)')
    print(f'  Embedding space: {data.get(\"embedding_space\", \"unknown\")}')
    print(f'  Uses visual projection: {data.get(\"uses_visual_projection\", False)}')
except Exception as e:
    print(f'  Could not parse recall results: {e}')
"
    fi
    
else
    echo "âš ï¸  No results to archive"
fi

# =============================================================================
# FINAL SUMMARY
# =============================================================================

echo ""
echo "ðŸŽ‰ FAIR COMPARISON EVALUATION COMPLETED!"
echo "========================================"

TOTAL_END_TIME=$(date +%s)
TOTAL_DURATION=$SECONDS

echo "ðŸ“Š Final Summary:"
echo "   Task 1 (Alignment): $([ $ALIGNMENT_EXIT_CODE -eq 0 ] && echo "âœ… SUCCESS" || echo "âŒ FAILED")"
echo "   Task 2 (Recall): $([ $RECALL_EXIT_CODE -eq 0 ] && echo "âœ… SUCCESS" || echo "âŒ FAILED")"
echo "   Total samples evaluated: $MAX_SAMPLES"
echo "   Results location: $PERSISTENT_RESULTS_DIR"
echo ""
echo "ðŸŽ¯ Evaluation Improvements Applied:"
echo "   âœ… CLIP visual projection used for both methods"
echo "   âœ… Fair comparison in CLIP's aligned 768-dim space"
echo "   âœ… Literature-compliant evaluation methodology"
echo "   âœ… Meaningful performance differences captured"

if [ $ALIGNMENT_EXIT_CODE -eq 0 ] && [ $RECALL_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "ðŸš€ Both evaluations completed successfully with fair comparison!"
    echo "ðŸ“Š Check the results in: $PERSISTENT_RESULTS_DIR"
    echo "ðŸ“ˆ Summary files:"
    echo "   - alignment_summary.json (Task 1 results with fair comparison)"
    echo "   - recall_summary.json (Task 2 results with fair comparison)"
    echo "   - evaluation_summary.txt (Overall summary with improvements)"
    echo ""
    echo "ðŸ”¬ Technical Details:"
    echo "   â€¢ Both image embeddings projected to CLIP's aligned 768-dim space"
    echo "   â€¢ Text embeddings already in CLIP's aligned 768-dim space"
    echo "   â€¢ Results now reflect actual model performance differences"
    echo "   â€¢ Baseline (CLIP vision) properly established with projection"
    
    OVERALL_EXIT_CODE=0
else
    echo ""
    echo "âš ï¸  Some evaluations failed. Check the logs for details."
    echo "ðŸ“ Logs location: ./slurm_out/evaluation_${SLURM_JOB_ID}.{out,err}"
    
    OVERALL_EXIT_CODE=1
fi

echo ""
echo "ðŸŽ‰ Job completed at: $(date)"
echo "â±ï¸  Total runtime: $TOTAL_DURATION seconds"
echo "ðŸŽ¯ Fair comparison evaluation ensures meaningful results!"

exit $OVERALL_EXIT_CODE