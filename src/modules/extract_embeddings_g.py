#!/usr/bin/env python3
"""
Updated BLIP3-o Embedding Extraction with CLS + Patches Support
src/modules/extract_embeddings_g.py

CHANGES:
1. Extract CLS token + patches: [B, 257, dim] (CLS first, then 256 patches)
2. Support both 256 (patch only) and 257 (CLS + patch) modes
3. Flexible shard selection for training
4. Better mode detection and validation
"""

import sys
import os
import torch
import torch.nn.functional as F
from transformers import CLIPProcessor, CLIPModel, AutoModel, CLIPImageProcessor
import pickle
from tqdm import tqdm
import numpy as np
from pathlib import Path
import gc
import psutil
import time
import glob
import json
import shutil
import argparse

def setup_paths():
    """Setup paths for project structure"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    
    # Add import paths
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(project_root / "src"))
    sys.path.insert(0, str(project_root / "src" / "data_hand"))
    sys.path.insert(0, str(project_root / "src" / "utils"))
    
    return project_root

def setup_temp_manager():
    """Setup temp manager for structured directory management."""
    try:
        from src.modules.utils.temp_manager import setup_snellius_environment
        manager = setup_snellius_environment("blip3o_workspace")
        return manager
    except ImportError:
        print("‚ö†Ô∏è  Temp manager not available, using fallback directories")
        return None

def get_memory_usage():
    """Get current memory usage in GB"""
    try:
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        return memory_mb / 1024
    except:
        return 0.0

def cleanup_memory():
    """Aggressive memory cleanup"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def load_models(device):
    """Load CLIP and EVA-CLIP models"""
    print("üì¶ Loading models...")
    
    # Load CLIP ViT-L/14
    print("   Loading CLIP ViT-L/14...")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    clip_model = CLIPModel.from_pretrained(
        "openai/clip-vit-large-patch14",
        torch_dtype=torch.float16
    ).to(device)
    clip_model.eval()
    
    # Load EVA-CLIP-8B
    print("   Loading EVA-CLIP-8B...")
    eva_model = AutoModel.from_pretrained(
        "BAAI/EVA-CLIP-8B", 
        trust_remote_code=True,
        torch_dtype=torch.float16
    ).to(device)
    
    eva_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
    eva_model.eval()
    
    cleanup_memory()
    
    print("‚úÖ Models loaded successfully")
    print(f"üíæ Memory usage after loading: {get_memory_usage():.2f} GB")
    
    return clip_processor, clip_model, eva_processor, eva_model

def extract_clip_features_with_cls(images, processor, model, device, include_cls=True):
    """
    Extract CLIP ViT-L/14 features with CLS token + patches
    
    Args:
        images: List of PIL images
        processor: CLIP processor
        model: CLIP model
        device: Device
        include_cls: Whether to include CLS token (default: True)
        
    Returns:
        Features tensor [B, 257, 1024] if include_cls else [B, 256, 1024]
    """
    features = []
    
    for img in images:
        inputs = processor(images=img, return_tensors="pt")
        inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device) 
                 for k, v in inputs.items()}
        
        with torch.no_grad():
            vision_outputs = model.vision_model(
                pixel_values=inputs['pixel_values'],
                output_hidden_states=True,
                return_dict=True
            )
            
            # Get all hidden states (CLS + patches)
            # ViT-L/14 outputs: [1, 257, 1024] where [0] is CLS, [1:257] are patches
            if include_cls:
                # Keep CLS token + patches: [1, 257, 1024]
                all_embeddings = vision_outputs.last_hidden_state  # [1, 257, 1024]
                expected_tokens = 257
            else:
                # Remove CLS token (patches only): [1, 256, 1024]
                all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, 1024]
                expected_tokens = 256
            
            batch_size, num_tokens, hidden_dim = all_embeddings.shape
            
            # Validate dimensions
            assert hidden_dim == 1024, f"Expected CLIP 1024-dim, got {hidden_dim}"
            assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
            
            # Convert to float32 and move to CPU
            features.append(all_embeddings.squeeze().cpu().float())
            
            # Clear GPU memory
            del vision_outputs, all_embeddings
    
    return torch.stack(features)

def extract_eva_features_with_cls(images, processor, model, device, include_cls=True):
    """
    Extract EVA-CLIP-8B features with CLS token + patches
    
    Args:
        images: List of PIL images
        processor: EVA processor
        model: EVA model
        device: Device
        include_cls: Whether to include CLS token (default: True)
        
    Returns:
        Features tensor [B, 257, 4096] if include_cls else [B, 256, 4096]
    """
    features = []
    
    for img in images:
        inputs = processor(images=img, return_tensors="pt")
        pixel_values = inputs['pixel_values'].to(device).half()
        
        with torch.no_grad():
            vision_outputs = model.vision_model(
                pixel_values=pixel_values,
                output_hidden_states=True,
                return_dict=True
            )
            
            # Get all hidden states (CLS + patches)
            # EVA-CLIP outputs: [1, 257, hidden_dim] where [0] is CLS, [1:257] are patches
            if include_cls:
                # Keep CLS token + patches: [1, 257, hidden_dim]
                all_embeddings = vision_outputs.last_hidden_state  # [1, 257, hidden_dim]
                expected_tokens = 257
            else:
                # Remove CLS token (patches only): [1, 256, hidden_dim]
                all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, hidden_dim]
                expected_tokens = 256
            
            batch_size, num_tokens, hidden_dim = all_embeddings.shape
            
            # Validate dimensions
            assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
            
            # Convert to float32 and move to CPU
            features.append(all_embeddings.squeeze().cpu().float())
            
            # Clear GPU memory
            del vision_outputs, all_embeddings, pixel_values
    
    return torch.stack(features)

def find_data_files(temp_manager, max_shards=None):
    """Find downloaded tar files using temp manager."""
    if temp_manager:
        datasets_dir = temp_manager.get_datasets_dir()
        print(f"üîç Searching for dataset shards in: {datasets_dir}")
    else:
        # Fallback to old method
        print("üîç Searching for dataset shards (fallback method)...")
        if "TMPDIR" in os.environ:
            datasets_dir = Path(os.environ["TMPDIR"]) / "blip3o_data"
        elif "SCRATCH_SHARED" in os.environ:
            user = os.environ.get("USER", "user")
            datasets_dir = Path(os.environ["SCRATCH_SHARED"]) / user / "blip3o_data"
        else:
            datasets_dir = Path(__file__).parent.parent.parent / "data"
    
    # Look for tar files
    tar_files = list(datasets_dir.glob("*.tar"))
    if tar_files:
        tar_files.sort()  # Sort numerically
        tar_files = [str(f) for f in tar_files]
        
        # Limit number of shards if specified
        if max_shards is not None:
            tar_files = tar_files[:max_shards]
            print(f"üìä Limited to {max_shards} shards")
        
        print(f"   ‚úÖ Found {len(tar_files)} tar files")
        
        # Validate files exist and show details
        valid_files = []
        total_size_gb = 0
        
        print(f"\nüìä Validating found files...")
        for tar_file in tar_files:
            tar_path = Path(tar_file)
            if tar_path.exists():
                size_gb = tar_path.stat().st_size / (1024**3)
                total_size_gb += size_gb
                valid_files.append(tar_file)
                print(f"   ‚úÖ {tar_path.name}: {size_gb:.2f} GB")
            else:
                print(f"   ‚ùå Missing: {tar_file}")
        
        print(f"\nüéØ Using {len(valid_files)} tar files for extraction")
        print(f"üìä Total dataset size: {total_size_gb:.2f} GB")
        
        # Estimate samples
        estimated_samples = int(total_size_gb * 400000 / 1.0)  # Rough estimate
        print(f"üìä Estimated samples: ~{estimated_samples:,}")
        
        return valid_files
    
    raise FileNotFoundError(
        f"No TAR files found in {datasets_dir}!\n"
        "Please download dataset shards first:\n"
        "  python src/data_hand/download_data.py --shards 0 1 2 3 4 5 6 7 8 9\n"
    )

def process_single_tar(
    tar_file_path: str,
    shard_idx: int,
    clip_processor, clip_model, eva_processor, eva_model,
    device: torch.device,
    output_dir: Path,
    working_dir: Path,
    batch_size: int = 16,
    include_cls: bool = True,
    target_tokens: int = 257
) -> dict:
    """
    Process a single TAR file and save embeddings with CLS+patch support
    """
    
    print(f"\nüîÑ Processing shard {shard_idx}: {Path(tar_file_path).name}")
    print(f"   Mode: {'CLS+Patches' if include_cls else 'Patches only'} ({target_tokens} tokens)")
    
    # Expected output file path
    mode_suffix = "cls_patch" if include_cls else "patch_only"
    shard_filename = f"embeddings_shard_{shard_idx:05d}_{mode_suffix}.pkl"
    shard_path = output_dir / shard_filename
    
    # Check if this shard already exists
    if shard_path.exists():
        print(f"   ‚úÖ Shard {shard_idx} already exists: {shard_path}")
        file_size_mb = shard_path.stat().st_size / (1024 * 1024)
        
        try:
            with open(shard_path, 'rb') as f:
                existing_data = pickle.load(f)
            sample_count = len(existing_data.get('captions', []))
            
            return {
                'shard_idx': shard_idx,
                'total_samples': sample_count,
                'file_size_mb': file_size_mb,
                'processing_time': 0.0,
                'output_path': str(shard_path),
                'success': True,
                'skipped': True,
                'mode': mode_suffix,
                'tokens': target_tokens
            }
        except:
            print(f"   ‚ö†Ô∏è  Could not read existing file, will reprocess...")
            shard_path.unlink()
    
    # Import dataset (simplified fallback)
    try:
        import webdataset as wds
        from PIL import Image
        import io
        
        def decode_sample(sample):
            """Decode a sample from WebDataset"""
            try:
                # Get image
                for ext in ['jpg', 'jpeg', 'png', 'webp']:
                    if ext in sample:
                        image_data = sample[ext]
                        break
                else:
                    return None
                
                image = Image.open(io.BytesIO(image_data)).convert('RGB')
                
                # Get caption
                caption = ""
                for caption_key in ['txt', 'caption', 'text']:
                    if caption_key in sample:
                        caption_data = sample[caption_key]
                        if isinstance(caption_data, bytes):
                            caption = caption_data.decode('utf-8').strip()
                        else:
                            caption = str(caption_data).strip()
                        break
                
                key = sample.get('__key__', 'unknown')
                
                return {
                    'image': image,
                    'caption': caption,
                    'key': key,
                }
            except Exception as e:
                return None
        
        # Create WebDataset
        dataset = (wds.WebDataset([tar_file_path], empty_check=False)
                  .map(decode_sample)
                  .select(lambda x: x is not None))
        
        # Create dataloader
        def simple_collate(batch):
            images = [item['image'] for item in batch]
            captions = [item['caption'] for item in batch]
            keys = [item['key'] for item in batch]
            return {
                'image': images,
                'caption': captions,
                'key': keys
            }
        
        from torch.utils.data import DataLoader
        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=simple_collate)
        
        print(f"   ‚úÖ Created WebDataset dataloader")
    
    except Exception as e:
        print(f"   ‚ùå Failed to create dataloader: {e}")
        return {
            'shard_idx': shard_idx,
            'total_samples': 0,
            'success': False,
            'error': str(e)
        }
    
    # Storage for this shard's embeddings
    shard_clip_embeddings = []
    shard_eva_embeddings = []
    shard_captions = []
    shard_keys = []
    
    total_samples = 0
    start_time = time.time()
    
    print(f"   üìä Processing batches...")
    
    # Process all batches in this TAR file
    try:
        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Shard {shard_idx}", unit="batch")):
            try:
                images = batch['image']
                captions = batch['caption']
                keys = batch['key']
                
                # Extract features with CLS support
                clip_features = extract_clip_features_with_cls(
                    images, clip_processor, clip_model, device, include_cls=include_cls
                )
                cleanup_memory()
                
                eva_features = extract_eva_features_with_cls(
                    images, eva_processor, eva_model, device, include_cls=include_cls
                )
                cleanup_memory()
                
                # Validate shapes match target tokens
                assert clip_features.shape[1] == target_tokens, f"CLIP tokens: {clip_features.shape[1]} vs {target_tokens}"
                assert eva_features.shape[1] == target_tokens, f"EVA tokens: {eva_features.shape[1]} vs {target_tokens}"
                
                # Move to CPU and store
                shard_clip_embeddings.append(clip_features.cpu())
                shard_eva_embeddings.append(eva_features.cpu())
                shard_captions.extend(captions)
                shard_keys.extend(keys)
                
                total_samples += len(images)
                
                # Clear intermediate variables
                del clip_features, eva_features, images
                cleanup_memory()
                
                # Progress update
                if batch_idx % 10 == 0:
                    elapsed = time.time() - start_time
                    samples_per_sec = total_samples / elapsed if elapsed > 0 else 0
                    print(f"   Batch {batch_idx}: {total_samples} samples, {samples_per_sec:.1f} samples/sec")
            
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error processing batch {batch_idx}: {e}")
                continue
    
    except Exception as e:
        print(f"   ‚ùå Error iterating through dataloader: {e}")
        return {
            'shard_idx': shard_idx,
            'total_samples': 0,
            'success': False,
            'error': str(e)
        }
    
    # Consolidate embeddings for this shard
    if shard_clip_embeddings:
        print(f"   üîÑ Consolidating {total_samples} embeddings...")
        
        final_clip = torch.cat(shard_clip_embeddings, dim=0)
        final_eva = torch.cat(shard_eva_embeddings, dim=0)
        
        # Final validation
        assert final_clip.shape[1] == target_tokens, f"Final CLIP shape: {final_clip.shape}"
        assert final_eva.shape[1] == target_tokens, f"Final EVA shape: {final_eva.shape}"
        assert final_clip.shape[2] == 1024, f"CLIP dim: {final_clip.shape[2]}"
        
        # Create shard data
        shard_data = {
            'clip_blip3o_embeddings': final_clip,
            'eva_blip3o_embeddings': final_eva,
            'captions': shard_captions,
            'keys': shard_keys,
            'total_samples': total_samples,
            'shard_idx': shard_idx,
            'source_tar': tar_file_path,
            'config': {
                'clip_model': 'openai/clip-vit-large-patch14',
                'eva_model': 'BAAI/EVA-CLIP-8B',
                'clip_dim': 1024,
                'eva_dim': final_eva.shape[2],
                'tokens': target_tokens,
                'include_cls': include_cls,
                'mode': mode_suffix,
                'extraction_method': 'cls_patch_extraction_v3',
                'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if include_cls else ""}patch_v3',
                'extraction_time': time.time() - start_time,
                'cls_first': include_cls,  # CLS token is always first if included
                'patch_order': '16x16_row_major',  # Patches are in row-major order
            }
        }
        
        # Save shard data
        print(f"   üíæ Saving shard {shard_idx} to persistent storage...")
        
        try:
            with open(shard_path, 'wb') as f:
                pickle.dump(shard_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            file_size_mb = shard_path.stat().st_size / (1024 * 1024)
            
            print(f"   ‚úÖ Shard {shard_idx} completed:")
            print(f"      File: {shard_filename}")
            print(f"      Size: {file_size_mb:.1f} MB")
            print(f"      Samples: {total_samples}")
            print(f"      Mode: {mode_suffix} ({target_tokens} tokens)")
            print(f"      CLS+Patches: [0]=CLS, [1:257]=patches" if include_cls else "      Patches only: [0:256]=patches")
            print(f"      Time: {time.time() - start_time:.1f}s")
            
            # Clear memory
            del shard_clip_embeddings, shard_eva_embeddings, final_clip, final_eva
            cleanup_memory()
            
            return {
                'shard_idx': shard_idx,
                'total_samples': total_samples,
                'file_size_mb': file_size_mb,
                'processing_time': time.time() - start_time,
                'output_path': str(shard_path),
                'success': True,
                'mode': mode_suffix,
                'tokens': target_tokens,
                'include_cls': include_cls
            }
        
        except Exception as e:
            print(f"   ‚ùå Failed to save shard {shard_idx}: {e}")
            return {
                'shard_idx': shard_idx,
                'total_samples': total_samples,
                'success': False,
                'error': f'File save failed: {e}'
            }
    
    else:
        print(f"   ‚ùå No embeddings extracted from shard {shard_idx}")
        return {
            'shard_idx': shard_idx,
            'total_samples': 0,
            'success': False,
            'error': 'No embeddings extracted'
        }

def main():
    """Main extraction function with CLS+patch support."""
    parser = argparse.ArgumentParser(description="BLIP3-o Embedding Extraction with CLS+Patch support")
    parser.add_argument("--include_cls", action="store_true", default=False,
                       help="Include CLS token (257 tokens) or patches only (256 tokens)")
    parser.add_argument("--max_shards", type=int, default=None,
                       help="Maximum number of shards to process (for testing)")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="Batch size for processing")
    
    args = parser.parse_args()
    
    # Setup
    target_tokens = 257 if args.include_cls else 256
    mode_name = "CLS+Patches" if args.include_cls else "Patches only"
    
    print("üöÄ BLIP3-o Embedding Extraction with CLS+Patch Support")
    print("=" * 70)
    print(f"Mode: {mode_name} ({target_tokens} tokens)")
    print(f"CLS token: {'First token [0]' if args.include_cls else 'Not included'}")
    print(f"Patches: {'Tokens [1:257]' if args.include_cls else 'Tokens [0:256]'} (16x16 grid)")
    print(f"Max shards: {args.max_shards if args.max_shards else 'All'}")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA required for this script")
    
    device = torch.device('cuda')
    project_root = setup_paths()
    
    # Setup temp manager
    temp_manager = setup_temp_manager()
    
    if temp_manager:
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = temp_manager.create_embeddings_subdirectory(f"{mode_suffix}_{target_tokens}_tokens")
        working_dir = temp_manager.get_working_dir()
        temp_manager.setup_model_cache()
        
        print(f"‚úÖ Using structured temp management")
        print(f"üìÅ Embeddings dir: {embeddings_dir}")
        print(f"üìÅ Working dir: {working_dir}")
    else:
        # Fallback
        if "TMPDIR" in os.environ:
            base_temp = Path(os.environ["TMPDIR"])
        else:
            base_temp = Path("./temp")
        
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = base_temp / f"embeddings_{mode_suffix}"
        working_dir = base_temp / "working"
        
        embeddings_dir.mkdir(parents=True, exist_ok=True)
        working_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"‚ö†Ô∏è  Using fallback temp management")
        print(f"üìÅ Embeddings dir: {embeddings_dir}")
    
    print(f"üéÆ Using GPU: {torch.cuda.get_device_name(0)}")
    
    # Load models
    try:
        clip_processor, clip_model, eva_processor, eva_model = load_models(device)
    except Exception as e:
        print(f"‚ùå Failed to load models: {e}")
        return 1
    
    # Find TAR files
    try:
        tar_files = find_data_files(temp_manager, max_shards=args.max_shards)
    except Exception as e:
        print(f"‚ùå {e}")
        return 1
    
    print(f"üì§ Output directory: {embeddings_dir}")
    
    # Process each TAR file
    print(f"\nüîÑ Processing {len(tar_files)} TAR files...")
    
    processing_results = []
    total_samples_all = 0
    failed_shards = []
    
    for shard_idx, tar_file in enumerate(tar_files):
        print(f"\n" + "="*60)
        print(f"PROCESSING SHARD {shard_idx + 1}/{len(tar_files)}")
        print(f"="*60)
        
        result = process_single_tar(
            tar_file_path=tar_file,
            shard_idx=shard_idx,
            clip_processor=clip_processor,
            clip_model=clip_model,
            eva_processor=eva_processor,
            eva_model=eva_model,
            device=device,
            output_dir=embeddings_dir,
            working_dir=working_dir,
            batch_size=args.batch_size,
            include_cls=args.include_cls,
            target_tokens=target_tokens
        )
        
        if result and result['success']:
            processing_results.append(result)
            total_samples_all += result['total_samples']
            print(f"‚úÖ Shard {shard_idx} successful: {result['total_samples']} samples")
        else:
            failed_shards.append(shard_idx)
            print(f"‚ùå Shard {shard_idx} failed")
    
    # Create manifest
    manifest_data = {
        'total_shards': len(processing_results),
        'total_samples': total_samples_all,
        'extraction_mode': mode_name,
        'tokens_per_sample': target_tokens,
        'include_cls': args.include_cls,
        'cls_token_position': 0 if args.include_cls else None,
        'patch_tokens_range': [1, 257] if args.include_cls else [0, 256],
        'extraction_timestamp': time.time(),
        'shards': processing_results,
        'failed_shards': failed_shards,
        'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if args.include_cls else ""}patch_v3',
        'token_layout': {
            'cls_token': {'included': args.include_cls, 'position': 0 if args.include_cls else None},
            'patches': {
                'count': 256,
                'positions': [1, 257] if args.include_cls else [0, 256],
                'layout': '16x16 grid in row-major order'
            }
        },
        'usage': {
            'training_command_cls_patch': f'python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode cls_patch',
            'training_command_patch_only': f'python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode patch_only',
        }
    }
    
    manifest_path = embeddings_dir / "embeddings_manifest.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest_data, f, indent=2)
    
    # Final status
    print("\n" + "=" * 80)
    print("‚úÖ EMBEDDING EXTRACTION COMPLETED!")
    print("=" * 80)
    print(f"üìä SUMMARY:")
    print(f"   Mode: {mode_name} ({target_tokens} tokens)")
    print(f"   CLS token: {'Included at position [0]' if args.include_cls else 'Not included'}")
    print(f"   Patches: {'Positions [1:257]' if args.include_cls else 'Positions [0:256]'} (16x16)")
    print(f"   TAR files processed: {len(tar_files)}")
    print(f"   Successful shards: {len(processing_results)}")
    print(f"   Total samples: {total_samples_all:,}")
    print(f"   Embeddings location: {embeddings_dir}")
    print(f"   Manifest file: {manifest_path}")
    
    if len(processing_results) == len(tar_files):
        print(f"\nüéâ SUCCESS! All shards processed successfully!")
        print("Ready for BLIP3-o training!")
        print(f"\nUsage commands:")
        print(f"CLS+Patch mode (257 tokens):")
        print(f"  python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode cls_patch")
        print(f"Patch-only mode (256 tokens):")
        print(f"  python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode patch_only")
    
    print("=" * 80)
    
    return 0 if len(processing_results) == len(tar_files) else 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"‚ùå Critical error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)